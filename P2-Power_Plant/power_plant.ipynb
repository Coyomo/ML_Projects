{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e2fd85",
   "metadata": {},
   "source": [
    "Coyomo - ML_Project #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a894a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "752a24df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['AT', 'V', 'AP', 'RH', 'PE']\n",
      "\n",
      "Shape: (9568, 5)\n",
      "\n",
      "Summary statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>V</th>\n",
       "      <th>AP</th>\n",
       "      <th>RH</th>\n",
       "      <th>PE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9568.000000</td>\n",
       "      <td>9568.000000</td>\n",
       "      <td>9568.000000</td>\n",
       "      <td>9568.000000</td>\n",
       "      <td>9568.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19.618518</td>\n",
       "      <td>54.250021</td>\n",
       "      <td>1013.288871</td>\n",
       "      <td>73.308978</td>\n",
       "      <td>454.407820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.256412</td>\n",
       "      <td>13.993655</td>\n",
       "      <td>6.636609</td>\n",
       "      <td>16.094499</td>\n",
       "      <td>18.760047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-39.174839</td>\n",
       "      <td>-38.397358</td>\n",
       "      <td>959.607298</td>\n",
       "      <td>-53.091613</td>\n",
       "      <td>327.528030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.480000</td>\n",
       "      <td>41.670000</td>\n",
       "      <td>1009.077500</td>\n",
       "      <td>63.227500</td>\n",
       "      <td>439.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20.320000</td>\n",
       "      <td>52.080000</td>\n",
       "      <td>1012.950000</td>\n",
       "      <td>74.955000</td>\n",
       "      <td>451.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25.732500</td>\n",
       "      <td>66.540000</td>\n",
       "      <td>1017.320000</td>\n",
       "      <td>84.882500</td>\n",
       "      <td>468.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.344839</td>\n",
       "      <td>155.117358</td>\n",
       "      <td>1064.772702</td>\n",
       "      <td>187.691613</td>\n",
       "      <td>590.091970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AT            V           AP           RH           PE\n",
       "count  9568.000000  9568.000000  9568.000000  9568.000000  9568.000000\n",
       "mean     19.618518    54.250021  1013.288871    73.308978   454.407820\n",
       "std       8.256412    13.993655     6.636609    16.094499    18.760047\n",
       "min     -39.174839   -38.397358   959.607298   -53.091613   327.528030\n",
       "25%      13.480000    41.670000  1009.077500    63.227500   439.730000\n",
       "50%      20.320000    52.080000  1012.950000    74.955000   451.620000\n",
       "75%      25.732500    66.540000  1017.320000    84.882500   468.530000\n",
       "max      77.344839   155.117358  1064.772702   187.691613   590.091970"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('usina_with_outliers.csv')\n",
    "df.head()\n",
    "\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"\\nShape:\", df.shape)\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "display(df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e7ac6",
   "metadata": {},
   "source": [
    "## Q1: Outlier Detection and Removal using Cook's Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59cbe96",
   "metadata": {},
   "source": [
    "### Q1.1 - Model Choice and Justification\n",
    "- Why did you choose this model (Linear vs Ridge vs Lasso) for implementing Cook's Distance outlier detection?\n",
    "    - I chose to work with linear regression because Cook's Distance is based on linear regression. It measures how much the fitted values change when each observation is removed one at a time.\n",
    "- Why did you choose this library (Statsmodels vs scikit-learn)?\n",
    "    - Since the goal is to remove outliers and computationally determine which datapoints are outliers, we are going to need access to statistical summaries to understand how outliers affect the model. For this reason, statsmodels OLS will be the best option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b90ae",
   "metadata": {},
   "source": [
    "### Q1.2 - Identify outliers, remove them, and export clean CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3ee8630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for OLS Diagnostics and Plotting\n",
    "def fit_ols_diagnostics(X, y):\n",
    "    \"\"\"Fit OLS and return (model, diagnostics dataframe).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n,) or (n, p)\n",
    "        Feature matrix (without intercept column).\n",
    "    y : array-like, shape (n,)\n",
    "        Target vector.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y).reshape(-1)\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "\n",
    "    # Add intercept column for statsmodels\n",
    "    X_sm = sm.add_constant(X)\n",
    "    model = sm.OLS(y, X_sm).fit()\n",
    "\n",
    "    infl = OLSInfluence(model)\n",
    "    diag = pd.DataFrame({\n",
    "        \"y\": y,\n",
    "        \"y_hat\": model.fittedvalues,\n",
    "        \"residual\": model.resid,\n",
    "        \"leverage_hii\": infl.hat_matrix_diag,   # diagonal of Hat matrix H\n",
    "        \"cooks_D\": infl.cooks_distance[0]\n",
    "    })\n",
    "    return model, diag\n",
    "\n",
    "\n",
    "def plot_line_fit(x, y, model, title=\"\"):\n",
    "    \"\"\"Scatter + fitted line for 1D x.\"\"\"\n",
    "    x = np.asarray(x).reshape(-1)\n",
    "    order = np.argsort(x)\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    print(x.shape, y.shape)\n",
    "    plt.scatter(x, y)\n",
    "    x_sorted = x[order]\n",
    "\n",
    "    X_sm = sm.add_constant(x_sorted)\n",
    "    yhat_sorted = model.predict(X_sm)\n",
    "    plt.plot(x_sorted, yhat_sorted)\n",
    "\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_curve_fit(x, y, pipeline, title=\"\"):\n",
    "    \"\"\"Scatter + fitted curve for scikit-learn pipeline (1D x).\"\"\"\n",
    "    x = np.asarray(x).reshape(-1, 1)\n",
    "    y = np.asarray(y).reshape(-1)\n",
    "\n",
    "    # Create a smooth grid for plotting\n",
    "    grid = np.linspace(x.min(), x.max(), 200).reshape(-1, 1)\n",
    "    yhat = pipeline.predict(grid)\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(grid, yhat)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22a55086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target variable\n",
    "TARGET_COL = 'PE'\n",
    "features = df.drop(columns=[TARGET_COL]).copy()\n",
    "target = df[TARGET_COL].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "094a999b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>y_hat</th>\n",
       "      <th>residual</th>\n",
       "      <th>leverage_hii</th>\n",
       "      <th>cooks_D</th>\n",
       "      <th>is outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>370.64803</td>\n",
       "      <td>509.615713</td>\n",
       "      <td>-138.967683</td>\n",
       "      <td>0.037486</td>\n",
       "      <td>1.265702e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5670</th>\n",
       "      <td>364.32803</td>\n",
       "      <td>559.465487</td>\n",
       "      <td>-195.137457</td>\n",
       "      <td>0.018852</td>\n",
       "      <td>1.207862e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>541.43197</td>\n",
       "      <td>415.727283</td>\n",
       "      <td>125.704687</td>\n",
       "      <td>0.041370</td>\n",
       "      <td>1.152221e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>575.06197</td>\n",
       "      <td>408.386275</td>\n",
       "      <td>166.675695</td>\n",
       "      <td>0.024168</td>\n",
       "      <td>1.142049e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3477</th>\n",
       "      <td>358.96803</td>\n",
       "      <td>503.918492</td>\n",
       "      <td>-144.950462</td>\n",
       "      <td>0.030533</td>\n",
       "      <td>1.105591e+00</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>463.70000</td>\n",
       "      <td>463.686856</td>\n",
       "      <td>0.013144</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>7.718906e-11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4540</th>\n",
       "      <td>452.27000</td>\n",
       "      <td>452.258677</td>\n",
       "      <td>0.011323</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>6.102539e-11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>452.13000</td>\n",
       "      <td>452.124884</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>2.288271e-11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6353</th>\n",
       "      <td>463.46000</td>\n",
       "      <td>463.464109</td>\n",
       "      <td>-0.004109</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>1.607772e-11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>458.92000</td>\n",
       "      <td>458.917137</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>5.193910e-12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6697 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              y       y_hat    residual  leverage_hii       cooks_D  \\\n",
       "2397  370.64803  509.615713 -138.967683      0.037486  1.265702e+00   \n",
       "5670  364.32803  559.465487 -195.137457      0.018852  1.207862e+00   \n",
       "3919  541.43197  415.727283  125.704687      0.041370  1.152221e+00   \n",
       "3248  575.06197  408.386275  166.675695      0.024168  1.142049e+00   \n",
       "3477  358.96803  503.918492 -144.950462      0.030533  1.105591e+00   \n",
       "...         ...         ...         ...           ...           ...   \n",
       "5449  463.70000  463.686856    0.013144      0.000276  7.718906e-11   \n",
       "4540  452.27000  452.258677    0.011323      0.000294  6.102539e-11   \n",
       "960   452.13000  452.124884    0.005116      0.000539  2.288271e-11   \n",
       "6353  463.46000  463.464109   -0.004109      0.000587  1.607772e-11   \n",
       "5915  458.92000  458.917137    0.002863      0.000391  5.193910e-12   \n",
       "\n",
       "      is outlier  \n",
       "2397        True  \n",
       "5670        True  \n",
       "3919        True  \n",
       "3248        True  \n",
       "3477        True  \n",
       "...          ...  \n",
       "5449       False  \n",
       "4540       False  \n",
       "960        False  \n",
       "6353       False  \n",
       "5915       False  \n",
       "\n",
       "[6697 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1: Outlier Detection and Removal using Cook's Distance\n",
    "model_full, diag_full = fit_ols_diagnostics(X_train, y_train)\n",
    "# print(model_full.summary())\n",
    "\n",
    "# Identify influential points using Cook's Distance\n",
    "threshold = 4 / len(X_train)\n",
    "\n",
    "X_sm = sm.add_constant(X_train) # Adds a constant term to the predictors\n",
    "model_w_constants, diag_w_constants = fit_ols_diagnostics(X_sm, y_train)\n",
    "# print(model_w_constants.summary())\n",
    "\n",
    "diag_table = diag_w_constants.copy()\n",
    "diag_table[\"is outlier\"] = diag_table[\"cooks_D\"] > threshold\n",
    "\n",
    "diag_table.sort_values(by='cooks_D', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f9e123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.932\n",
      "Model:                            OLS   Adj. R-squared:                  0.932\n",
      "Method:                 Least Squares   F-statistic:                 2.261e+04\n",
      "Date:                Mon, 26 Jan 2026   Prob (F-statistic):               0.00\n",
      "Time:                        13:43:56   Log-Likelihood:                -19229.\n",
      "No. Observations:                6612   AIC:                         3.847e+04\n",
      "Df Residuals:                    6607   BIC:                         3.850e+04\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        453.1256     11.444     39.594      0.000     430.691     475.560\n",
      "x1            -1.9882      0.018   -111.007      0.000      -2.023      -1.953\n",
      "x2            -0.2305      0.008    -27.146      0.000      -0.247      -0.214\n",
      "x3             0.0632      0.011      5.689      0.000       0.041       0.085\n",
      "x4            -0.1528      0.005    -32.021      0.000      -0.162      -0.143\n",
      "==============================================================================\n",
      "Omnibus:                       36.513   Durbin-Watson:                   2.009\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               52.307\n",
      "Skew:                           0.018   Prob(JB):                     4.38e-12\n",
      "Kurtosis:                       3.434   Cond. No.                     2.13e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.13e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "Full-data:   b0 = 82.5912, b1 = -1.1106, b2 = -0.4240, b3 = 0.4116, b4 = -0.0067\n",
      "Cleaned-data: b0 = 453.1256, b1 = -1.9882, b2 = -0.2305, b3 = 0.0632, b4 = -0.1528\n"
     ]
    }
   ],
   "source": [
    "idx_remove = diag_table[diag_table[\"is outlier\"]].index.tolist()\n",
    "\n",
    "X_train_clean = np.delete(X_train, idx_remove, axis=0)\n",
    "y_train_clean = np.delete(y_train, idx_remove)\n",
    "\n",
    "model_clean, diag_clean = fit_ols_diagnostics(X_train_clean, y_train_clean)\n",
    "print(model_clean.summary())\n",
    "\n",
    "coef_full = np.asarray(model_full.params).reshape(-1)\n",
    "coef_clean = np.asarray(model_clean.params).reshape(-1)\n",
    "\n",
    "print(f\"Full-data:   b0 = {coef_full[0]:.4f}, b1 = {coef_full[1]:.4f}, b2 = {coef_full[2]:.4f}, b3 = {coef_full[3]:.4f}, b4 = {coef_full[4]:.4f}\")\n",
    "print(f\"Cleaned-data: b0 = {coef_clean[0]:.4f}, b1 = {coef_clean[1]:.4f}, b2 = {coef_clean[2]:.4f}, b3 = {coef_clean[3]:.4f}, b4 = {coef_clean[4]:.4f}\")\n",
    "\n",
    "df_clean = pd.DataFrame(X_train_clean, columns=features.columns)\n",
    "df_clean[TARGET_COL] = y_train_clean\n",
    "df_clean.to_csv('usina.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42d437",
   "metadata": {},
   "source": [
    "## Q2: Train/Test Evaluation Before vs After Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43078424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split outlier and cleaned data\n",
    "df_outlier = pd.read_csv('usina_with_outliers.csv')\n",
    "df_clean = pd.read_csv('usina.csv')\n",
    "X_o = df_outlier.drop(columns=[TARGET_COL]).values\n",
    "y_o = df_outlier[TARGET_COL].values\n",
    "X_c = df_clean.drop(columns=[TARGET_COL]).values\n",
    "y_c = df_clean[TARGET_COL].values\n",
    "\n",
    "X_train_outlier, X_test_outlier, y_train_outlier, y_test_outlier = train_test_split(X_o, y_o, test_size=0.3, random_state=42, shuffle = True)\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_c, y_c, test_size=0.3, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a153dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>train_mse</th>\n",
       "      <th>train_mae</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test_mae</th>\n",
       "      <th>test_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>clean</td>\n",
       "      <td>Linear</td>\n",
       "      <td>19.453496</td>\n",
       "      <td>3.571171</td>\n",
       "      <td>0.933362</td>\n",
       "      <td>20.166718</td>\n",
       "      <td>3.618280</td>\n",
       "      <td>0.928319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>clean</td>\n",
       "      <td>Ridge (alpha=0.01)</td>\n",
       "      <td>19.453496</td>\n",
       "      <td>3.571171</td>\n",
       "      <td>0.933362</td>\n",
       "      <td>20.166718</td>\n",
       "      <td>3.618280</td>\n",
       "      <td>0.928319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>clean</td>\n",
       "      <td>Ridge (alpha=0.1)</td>\n",
       "      <td>19.453496</td>\n",
       "      <td>3.571171</td>\n",
       "      <td>0.933362</td>\n",
       "      <td>20.166724</td>\n",
       "      <td>3.618281</td>\n",
       "      <td>0.928319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>clean</td>\n",
       "      <td>Ridge (alpha=1)</td>\n",
       "      <td>19.453496</td>\n",
       "      <td>3.571175</td>\n",
       "      <td>0.933362</td>\n",
       "      <td>20.166776</td>\n",
       "      <td>3.618289</td>\n",
       "      <td>0.928319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>clean</td>\n",
       "      <td>Lasso (alpha=0.01)</td>\n",
       "      <td>19.453500</td>\n",
       "      <td>3.571139</td>\n",
       "      <td>0.933362</td>\n",
       "      <td>20.166881</td>\n",
       "      <td>3.618290</td>\n",
       "      <td>0.928319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>clean</td>\n",
       "      <td>Ridge (alpha=10)</td>\n",
       "      <td>19.453498</td>\n",
       "      <td>3.571211</td>\n",
       "      <td>0.933362</td>\n",
       "      <td>20.167304</td>\n",
       "      <td>3.618377</td>\n",
       "      <td>0.928317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>clean</td>\n",
       "      <td>Ridge (alpha=100)</td>\n",
       "      <td>19.453674</td>\n",
       "      <td>3.571594</td>\n",
       "      <td>0.933361</td>\n",
       "      <td>20.172730</td>\n",
       "      <td>3.619265</td>\n",
       "      <td>0.928298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>clean</td>\n",
       "      <td>Lasso (alpha=0.1)</td>\n",
       "      <td>19.454078</td>\n",
       "      <td>3.571544</td>\n",
       "      <td>0.933360</td>\n",
       "      <td>20.176764</td>\n",
       "      <td>3.619849</td>\n",
       "      <td>0.928283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>clean</td>\n",
       "      <td>Lasso (alpha=1)</td>\n",
       "      <td>19.518117</td>\n",
       "      <td>3.580046</td>\n",
       "      <td>0.933140</td>\n",
       "      <td>20.332249</td>\n",
       "      <td>3.640196</td>\n",
       "      <td>0.927731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>clean</td>\n",
       "      <td>Lasso (alpha=10)</td>\n",
       "      <td>25.054273</td>\n",
       "      <td>4.040358</td>\n",
       "      <td>0.914176</td>\n",
       "      <td>26.558771</td>\n",
       "      <td>4.181254</td>\n",
       "      <td>0.905599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>outlier</td>\n",
       "      <td>Linear</td>\n",
       "      <td>123.384207</td>\n",
       "      <td>5.198679</td>\n",
       "      <td>0.650171</td>\n",
       "      <td>125.113389</td>\n",
       "      <td>5.052458</td>\n",
       "      <td>0.642574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>outlier</td>\n",
       "      <td>Ridge (alpha=0.01)</td>\n",
       "      <td>123.384207</td>\n",
       "      <td>5.198679</td>\n",
       "      <td>0.650171</td>\n",
       "      <td>125.113389</td>\n",
       "      <td>5.052458</td>\n",
       "      <td>0.642574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>outlier</td>\n",
       "      <td>Ridge (alpha=0.1)</td>\n",
       "      <td>123.384207</td>\n",
       "      <td>5.198680</td>\n",
       "      <td>0.650171</td>\n",
       "      <td>125.113392</td>\n",
       "      <td>5.052459</td>\n",
       "      <td>0.642574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>outlier</td>\n",
       "      <td>Ridge (alpha=1)</td>\n",
       "      <td>123.384207</td>\n",
       "      <td>5.198685</td>\n",
       "      <td>0.650171</td>\n",
       "      <td>125.113421</td>\n",
       "      <td>5.052463</td>\n",
       "      <td>0.642574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>outlier</td>\n",
       "      <td>Ridge (alpha=10)</td>\n",
       "      <td>123.384207</td>\n",
       "      <td>5.198737</td>\n",
       "      <td>0.650171</td>\n",
       "      <td>125.113705</td>\n",
       "      <td>5.052513</td>\n",
       "      <td>0.642573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>outlier</td>\n",
       "      <td>Lasso (alpha=0.01)</td>\n",
       "      <td>123.384210</td>\n",
       "      <td>5.198858</td>\n",
       "      <td>0.650171</td>\n",
       "      <td>125.115332</td>\n",
       "      <td>5.052626</td>\n",
       "      <td>0.642568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>outlier</td>\n",
       "      <td>Ridge (alpha=100)</td>\n",
       "      <td>123.384213</td>\n",
       "      <td>5.199261</td>\n",
       "      <td>0.650171</td>\n",
       "      <td>125.116548</td>\n",
       "      <td>5.053003</td>\n",
       "      <td>0.642565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>outlier</td>\n",
       "      <td>Lasso (alpha=0.1)</td>\n",
       "      <td>123.384607</td>\n",
       "      <td>5.201739</td>\n",
       "      <td>0.650169</td>\n",
       "      <td>125.134495</td>\n",
       "      <td>5.055348</td>\n",
       "      <td>0.642514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>outlier</td>\n",
       "      <td>Lasso (alpha=1)</td>\n",
       "      <td>123.421151</td>\n",
       "      <td>5.229507</td>\n",
       "      <td>0.650066</td>\n",
       "      <td>125.338356</td>\n",
       "      <td>5.083092</td>\n",
       "      <td>0.641931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>clean</td>\n",
       "      <td>Lasso (alpha=100)</td>\n",
       "      <td>131.895679</td>\n",
       "      <td>9.486983</td>\n",
       "      <td>0.548190</td>\n",
       "      <td>127.911543</td>\n",
       "      <td>9.324437</td>\n",
       "      <td>0.545350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>outlier</td>\n",
       "      <td>Lasso (alpha=10)</td>\n",
       "      <td>126.141514</td>\n",
       "      <td>5.540418</td>\n",
       "      <td>0.642353</td>\n",
       "      <td>128.902136</td>\n",
       "      <td>5.408803</td>\n",
       "      <td>0.631750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>outlier</td>\n",
       "      <td>Lasso (alpha=100)</td>\n",
       "      <td>234.655937</td>\n",
       "      <td>11.285489</td>\n",
       "      <td>0.334683</td>\n",
       "      <td>233.064336</td>\n",
       "      <td>11.258653</td>\n",
       "      <td>0.334178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    variant               model   train_mse  train_mae  train_r2    test_mse  \\\n",
       "11    clean              Linear   19.453496   3.571171  0.933362   20.166718   \n",
       "12    clean  Ridge (alpha=0.01)   19.453496   3.571171  0.933362   20.166718   \n",
       "14    clean   Ridge (alpha=0.1)   19.453496   3.571171  0.933362   20.166724   \n",
       "16    clean     Ridge (alpha=1)   19.453496   3.571175  0.933362   20.166776   \n",
       "13    clean  Lasso (alpha=0.01)   19.453500   3.571139  0.933362   20.166881   \n",
       "18    clean    Ridge (alpha=10)   19.453498   3.571211  0.933362   20.167304   \n",
       "20    clean   Ridge (alpha=100)   19.453674   3.571594  0.933361   20.172730   \n",
       "15    clean   Lasso (alpha=0.1)   19.454078   3.571544  0.933360   20.176764   \n",
       "17    clean     Lasso (alpha=1)   19.518117   3.580046  0.933140   20.332249   \n",
       "19    clean    Lasso (alpha=10)   25.054273   4.040358  0.914176   26.558771   \n",
       "0   outlier              Linear  123.384207   5.198679  0.650171  125.113389   \n",
       "1   outlier  Ridge (alpha=0.01)  123.384207   5.198679  0.650171  125.113389   \n",
       "3   outlier   Ridge (alpha=0.1)  123.384207   5.198680  0.650171  125.113392   \n",
       "5   outlier     Ridge (alpha=1)  123.384207   5.198685  0.650171  125.113421   \n",
       "7   outlier    Ridge (alpha=10)  123.384207   5.198737  0.650171  125.113705   \n",
       "2   outlier  Lasso (alpha=0.01)  123.384210   5.198858  0.650171  125.115332   \n",
       "9   outlier   Ridge (alpha=100)  123.384213   5.199261  0.650171  125.116548   \n",
       "4   outlier   Lasso (alpha=0.1)  123.384607   5.201739  0.650169  125.134495   \n",
       "6   outlier     Lasso (alpha=1)  123.421151   5.229507  0.650066  125.338356   \n",
       "21    clean   Lasso (alpha=100)  131.895679   9.486983  0.548190  127.911543   \n",
       "8   outlier    Lasso (alpha=10)  126.141514   5.540418  0.642353  128.902136   \n",
       "10  outlier   Lasso (alpha=100)  234.655937  11.285489  0.334683  233.064336   \n",
       "\n",
       "     test_mae   test_r2  \n",
       "11   3.618280  0.928319  \n",
       "12   3.618280  0.928319  \n",
       "14   3.618281  0.928319  \n",
       "16   3.618289  0.928319  \n",
       "13   3.618290  0.928319  \n",
       "18   3.618377  0.928317  \n",
       "20   3.619265  0.928298  \n",
       "15   3.619849  0.928283  \n",
       "17   3.640196  0.927731  \n",
       "19   4.181254  0.905599  \n",
       "0    5.052458  0.642574  \n",
       "1    5.052458  0.642574  \n",
       "3    5.052459  0.642574  \n",
       "5    5.052463  0.642574  \n",
       "7    5.052513  0.642573  \n",
       "2    5.052626  0.642568  \n",
       "9    5.053003  0.642565  \n",
       "4    5.055348  0.642514  \n",
       "6    5.083092  0.641931  \n",
       "21   9.324437  0.545350  \n",
       "8    5.408803  0.631750  \n",
       "10  11.258653  0.334178  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Linear, Ridge, and Lasso Regression Baseline Models\n",
    "def fit_and_report_basic_models():\n",
    "    models = {\n",
    "        \"Linear\": LinearRegression(),\n",
    "        \"Ridge (alpha=0.01)\": Ridge(alpha=0.01, random_state=42),\n",
    "        \"Lasso (alpha=0.01)\": Lasso(alpha=0.01, random_state=42),\n",
    "        \"Ridge (alpha=0.1)\": Ridge(alpha=0.1, random_state=42),\n",
    "        \"Lasso (alpha=0.1)\": Lasso(alpha=0.1, random_state=42),\n",
    "        \"Ridge (alpha=1)\": Ridge(alpha=1, random_state=42),\n",
    "        \"Lasso (alpha=1)\": Lasso(alpha=1, random_state=42),\n",
    "        \"Ridge (alpha=10)\": Ridge(alpha=10, random_state=42),\n",
    "        \"Lasso (alpha=10)\": Lasso(alpha=10, random_state=42),\n",
    "        \"Ridge (alpha=100)\": Ridge(alpha=100, random_state=42),\n",
    "        \"Lasso (alpha=100)\": Lasso(alpha=100, random_state=42)\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    for variant in [\"outlier\", \"clean\"]:\n",
    "        if variant == \"outlier\":\n",
    "            X_train, y_train = X_train_outlier, y_train_outlier\n",
    "            X_test, y_test = X_test_outlier, y_test_outlier\n",
    "        else:\n",
    "            X_train, y_train = X_train_clean, y_train_clean\n",
    "            X_test, y_test = X_test_clean, y_test_clean\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train.ravel())\n",
    "            pred_tr = model.predict(X_train).reshape(-1, 1)\n",
    "            pred_te = model.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "            mtr = eval_metrics(y_train, pred_tr)\n",
    "            mte = eval_metrics(y_test, pred_te)\n",
    "\n",
    "            rows.append({\n",
    "                \"variant\": variant,\n",
    "                \"model\": name,\n",
    "                **{f\"train_{k}\": v for k, v in mtr.items()},\n",
    "                **{f\"test_{k}\": v for k, v in mte.items()}\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "            \n",
    "\n",
    "def eval_metrics(y_true, y_pred):\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    return {\n",
    "        \"mse\": mean_squared_error(y_true, y_pred),\n",
    "        \"mae\": mean_absolute_error(y_true, y_pred),\n",
    "        \"r2\": r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "q2_results = fit_and_report_basic_models()\n",
    "display(q2_results.sort_values([\"test_mse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65451f8",
   "metadata": {},
   "source": [
    "### Discussion Questions:\n",
    "\n",
    "- Do outliers change train error? Test error?\n",
    "    - Yes, there is a large mse training error difference, while the different in the mae is not so significant. As for test error the same pattern arises, an order of magnitude difference between the test mse of the clean and outlier dataset and a somewhat similar test mae between the clean and outlier dataset. As for the R^2 value, there is a high and reliable value for all clean dataset models, and poor results with the outlier-trained models.\n",
    "- Which dataset (with outliers vs without outliers) shows better generalization?\n",
    "    - The dataset without outliers shows better generalization. This is due to the fact that without the presence of outliers there is no significant leverage or residual which means that the model does not have to generalize for those extreme (and noisy) cases. Although, there is one model that was built on the clean dataset whose performance is near the bottom and being worse than models trained on the outlier dataset. This shows that shoowing the correct lambda value (learning importance) is integral to a good model.\n",
    "- Do Ridge/Lasso appear to help relative to standard linear regression?\n",
    "    - They don't seem to help, but they are not far behind linear regression in terms of error minimization. Linear and Ridge perform extremely similarly with Linear regression (especially in the low lambda values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc7075",
   "metadata": {},
   "source": [
    "## Q3: Reliability of Coefficients (Use Outlier-Removed Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4413a",
   "metadata": {},
   "source": [
    "### Discussion Questions:\n",
    "\n",
    "- Why did you choose this model (Linear vs Ridge vs Lasso)?\n",
    "    - I will use Linear Regression on cleaned data for further analysis due to the fact that it had the best performance in the baseline testing.\n",
    "- Why did you choose this library (Statsmodels vs scikit-learn)?\n",
    "    - I chose statsmodels OLS because the calculation of coefficients requires a detailed summary of the regression results which statsmodels' OLS provides.\n",
    "- Would you scale IVs and/or DV for this coefficient reliability task?\n",
    "    - I would scaled the IVs for this coefficient reliability task. I would scale the IVs because there are data entries that are at different orders of magnitude and scaling them would allow for a better understanding of the relative importance of each IV. As for the DV, I don't believe it matters since there is only one DV that is being derived from the IVs; scaling the DV would only change the scale of all the coefficients of each IV but it would not change the relative difference between them. Regardless, scaling the DV is best practice in case the DV values are skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will use Linear Regression on cleaned data for further analysis due to it "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
